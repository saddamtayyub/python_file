{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#overview\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    " #spark is an existing SparkSession\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()\n",
    "\n",
    "      #Select only the \"name\" column\n",
    "df.select(\"name\").show()\n",
    "# +-------+\n",
    "# |   name|\n",
    "# +-------+\n",
    "# |Michael|\n",
    "# |   Andy|\n",
    "# | Justin|\n",
    "# +-------+\n",
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "# +-------+---------+\n",
    "# |   name|(age + 1)|\n",
    "# +-------+---------+\n",
    "# |Michael|     null|\n",
    "# |   Andy|       31|\n",
    "# | Justin|       20|\n",
    "# +-------+---------+\n",
    "\n",
    "\n",
    "    # Select people older than 21\n",
    "df.filter(df['age'] > 21).show()\n",
    "# +---+----+\n",
    "# |age|name|\n",
    "# +---+----+\n",
    "# | 30|Andy|\n",
    "\n",
    "\n",
    "\n",
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "# +----+-----+\n",
    "# | age|count|\n",
    "# +----+-----+\n",
    "# |  19|    1|\n",
    "# |null|    1|\n",
    "# |  30|    1|\n",
    "\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()\n",
    "# +----+-------+\n",
    "# | age|   name|\n",
    "# +----+-------+\n",
    "# |null|Michael|\n",
    "# |  30|   Andy|\n",
    "# |  19| Justin|\n",
    "\n",
    "\n",
    "\n",
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "# +----+-------+\n",
    "# | age|   name|\n",
    "# +----+-------+\n",
    "# |null|Michael|\n",
    "# |  30|   Andy|\n",
    "# |  19| Justin|\n",
    "# +----+-------+\n",
    "\n",
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "# +----+-------+\n",
    "# | age|   name|\n",
    "# +----+-------+\n",
    "# |null|Michael|\n",
    "# |  30|   Andy|\n",
    "# |  19| Justin|\n",
    "# +----+-------+\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n",
    "# Name: Justin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"name age\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "results.show()\n",
    "# +-------+\n",
    "# |   name|\n",
    "# +-------+\n",
    "# |Michael|\n",
    "# |   Andy|\n",
    "# | Justin|\n",
    "# +-------+\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
